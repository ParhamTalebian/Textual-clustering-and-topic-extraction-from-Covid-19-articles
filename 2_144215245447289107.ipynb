{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9CwaW0b0CZoU"
   },
   "source": [
    "<font face=\"B Mitra\" size=4>\n",
    "<div dir=rtl align=center>\n",
    "<br>\n",
    "<img src=\"https://aut.ac.ir/templates/tmpl_modern01/images/logo_fa.png\" alt=\"Amirkabir University Logo\" width=\"100\">\n",
    "<br>\n",
    "<font size=6>\n",
    "<b>پروژه سوم داده کاوی</b>\n",
    "<br>\n",
    "<b><font size=5> استاد درس: دکتر فاطمه شاکری</b>\n",
    "<hr>\n",
    "</div>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imKU8ry_DIVS"
   },
   "source": [
    "<font face=\"B Mitra\">\n",
    "<div dir=rtl>\n",
    "<font size=5>\n",
    "کتابخانه های موردنیاز را در این بخش بارگذاری کنید.\n",
    "</p>\n",
    "</font>\n",
    "</div>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wqiVDGR55Wfk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.0.9)\n",
      "Requirement already satisfied: six in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langdetect) (1.16.0)\n",
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_sci_lg-0.4.0/en_core_sci_lg-0.4.0.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: HTTP error 404 while getting https://github.com/explosion/spacy-models/releases/download/en_core_sci_lg-0.4.0/en_core_sci_lg-0.4.0.tar.gz\n",
      "ERROR: Could not install requirement https://github.com/explosion/spacy-models/releases/download/en_core_sci_lg-0.4.0/en_core_sci_lg-0.4.0.tar.gz because of HTTP error 404 Client Error: Not Found for url: https://github.com/explosion/spacy-models/releases/download/en_core_sci_lg-0.4.0/en_core_sci_lg-0.4.0.tar.gz for URL https://github.com/explosion/spacy-models/releases/download/en_core_sci_lg-0.4.0/en_core_sci_lg-0.4.0.tar.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (24.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (70.0.0)\n",
      "Requirement already satisfied: wheel in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.43.0)\n",
      "Requirement already satisfied: spacy in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.7.4)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.7.5-cp312-cp312-win_amd64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.7.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (70.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.26.2)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.3 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (6.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.1)\n",
      "Downloading spacy-3.7.5-cp312-cp312-win_amd64.whl (11.7 MB)\n",
      "   ---------------------------------------- 0.0/11.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.7 MB 330.3 kB/s eta 0:00:36\n",
      "   ---------------------------------------- 0.0/11.7 MB 330.3 kB/s eta 0:00:36\n",
      "   ---------------------------------------- 0.0/11.7 MB 281.8 kB/s eta 0:00:42\n",
      "   ---------------------------------------- 0.1/11.7 MB 273.8 kB/s eta 0:00:43\n",
      "   ---------------------------------------- 0.1/11.7 MB 350.1 kB/s eta 0:00:34\n",
      "   ---------------------------------------- 0.1/11.7 MB 400.9 kB/s eta 0:00:29\n",
      "   ---------------------------------------- 0.1/11.7 MB 448.2 kB/s eta 0:00:26\n",
      "    --------------------------------------- 0.2/11.7 MB 456.4 kB/s eta 0:00:26\n",
      "    --------------------------------------- 0.2/11.7 MB 461.0 kB/s eta 0:00:25\n",
      "    --------------------------------------- 0.2/11.7 MB 515.5 kB/s eta 0:00:23\n",
      "    --------------------------------------- 0.2/11.7 MB 515.5 kB/s eta 0:00:23\n",
      "   - -------------------------------------- 0.4/11.7 MB 637.7 kB/s eta 0:00:18\n",
      "   - -------------------------------------- 0.5/11.7 MB 723.0 kB/s eta 0:00:16\n",
      "   - -------------------------------------- 0.5/11.7 MB 736.0 kB/s eta 0:00:16\n",
      "   - -------------------------------------- 0.5/11.7 MB 749.3 kB/s eta 0:00:15\n",
      "   - -------------------------------------- 0.6/11.7 MB 737.3 kB/s eta 0:00:16\n",
      "   -- ------------------------------------- 0.7/11.7 MB 839.1 kB/s eta 0:00:14\n",
      "   -- ------------------------------------- 0.8/11.7 MB 927.5 kB/s eta 0:00:12\n",
      "   --- ------------------------------------ 0.9/11.7 MB 978.0 kB/s eta 0:00:12\n",
      "   --- ------------------------------------ 1.0/11.7 MB 1.0 MB/s eta 0:00:11\n",
      "   --- ------------------------------------ 1.1/11.7 MB 1.1 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 1.2/11.7 MB 1.1 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 1.3/11.7 MB 1.2 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 1.4/11.7 MB 1.2 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 1.5/11.7 MB 1.3 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 1.6/11.7 MB 1.3 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 1.7/11.7 MB 1.4 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 1.8/11.7 MB 1.4 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 1.9/11.7 MB 1.4 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 2.0/11.7 MB 1.5 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 2.1/11.7 MB 1.5 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 2.2/11.7 MB 1.5 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 2.5/11.7 MB 1.6 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 2.8/11.7 MB 1.6 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 3.0/11.7 MB 1.7 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 3.2/11.7 MB 1.7 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 3.3/11.7 MB 1.7 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 3.6/11.7 MB 1.7 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 3.7/11.7 MB 1.7 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 3.9/11.7 MB 1.8 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 4.1/11.7 MB 1.8 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 4.2/11.7 MB 1.7 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 4.4/11.7 MB 1.8 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 4.7/11.7 MB 1.8 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 4.8/11.7 MB 1.8 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 5.1/11.7 MB 1.8 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 5.2/11.7 MB 1.8 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 5.4/11.7 MB 1.8 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 5.6/11.7 MB 1.9 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 5.8/11.7 MB 1.9 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 5.9/11.7 MB 1.9 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 6.0/11.7 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 6.2/11.7 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 6.4/11.7 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 6.6/11.7 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 6.7/11.7 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 6.9/11.7 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 7.1/11.7 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 7.3/11.7 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 7.4/11.7 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 7.6/11.7 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 7.7/11.7 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 7.9/11.7 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 8.0/11.7 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 8.2/11.7 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 8.3/11.7 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 8.5/11.7 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 8.6/11.7 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 8.8/11.7 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 8.9/11.7 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 9.2/11.7 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 9.3/11.7 MB 2.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.4/11.7 MB 2.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.6/11.7 MB 2.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 9.7/11.7 MB 2.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.9/11.7 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.1/11.7 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.1/11.7 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.3/11.7 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.7 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.7 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.7 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.7 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.7 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.7 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.7 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.7 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.7 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.7 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.7 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.7 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.7 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.7 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.7 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.7 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.7 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.7 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.7 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.7 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.7 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.7 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.7 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.7 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.7 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.7 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.7/11.7 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.1/11.7 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.5/11.7 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.5/11.7 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.6/11.7 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.6/11.7 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.7/11.7 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.7/11.7 MB 1.6 MB/s eta 0:00:00\n",
      "Installing collected packages: spacy\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.7.4\n",
      "    Uninstalling spacy-3.7.4:\n",
      "      Successfully uninstalled spacy-3.7.4\n",
      "Successfully installed spacy-3.7.5\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect\n",
    "\n",
    "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_sci_lg-0.4.0/en_core_sci_lg-0.4.0.tar.gz\n",
    "\n",
    "!pip install -U pip setuptools wheel\n",
    "\n",
    "!pip install -U spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z08MXPBzY_09"
   },
   "source": [
    "<font face=\"B Mitra\">\n",
    "<div dir=rtl>\n",
    "<font size=5>\n",
    "نصب پکیج <code>spacy</code> ممکن است وابسته به سیستم‌عامل و پکیج‌منیجر شما نیاز به دستوری متفاوت داشته‌باشد. دستور مناسب را می‌تواند با استفاده از\n",
    "<a href=\"https://spacy.io/usage\">این لینک</a> بیابید.\n",
    "</p>\n",
    "</font>\n",
    "</div>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hF_lqFaDUMGY"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'en_core_sci_lg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlang\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01men\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstop_words\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m STOP_WORDS\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01men_core_sci_lg\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Used to import list of punctuations\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'en_core_sci_lg'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "import seaborn as sns\n",
    "\n",
    "# Used to draw a progress bar for longer method calls\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Used to detect language used in each document\n",
    "from langdetect import detect\n",
    "from langdetect import DetectorFactory\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# Pre-trained natural language processing pipeline for biomedical use\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import en_core_sci_lg\n",
    "\n",
    "# Used to import list of punctuations\n",
    "import string\n",
    "\n",
    "# Feature extraction (text vectorizers)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Clustering and evaluation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Topic modeling\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Srip_iHlUMGb"
   },
   "source": [
    "<font face=\"B Mitra\">\n",
    "<div dir=rtl>\n",
    "<font size=6>\n",
    "<b> لود کردن مجموعه داده </b>\n",
    "<br>\n",
    "<p align=\"justify\">\n",
    "<font size=5>\n",
    "فایل مجموعه داده در <a href=\"https://drive.google.com/file/d/15E8FLX0C-6qpK-lDBEQJXw00Lsdcvjae/view?usp=sharing\">گوگل درایو</a> در اختیار شما قرار گرفته است:\n",
    "<br>\n",
    "همچنین در صورت استفاده از گوگل کولب با استفاده از دستور زیر میتوانید مجموعه داده را از گوگل درایو در نوتبوک خود دانلود کنید.\n",
    "</p>\n",
    "</font>\n",
    "</div>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mY9evTyRQA82"
   },
   "source": [
    "!gdown 15E8FLX0C-6qpK-lDBEQJXw00Lsdcvjae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ur5VMzf4QD92"
   },
   "source": [
    "<font face=\"B Mitra\">\n",
    "<div dir=rtl>\n",
    "<font size=5>\n",
    "در شروع کار،دیتافریم موردنظر خود را ایجاد کردیم و مقادیر خالی را با space جایگزین کردیم.\n",
    "دلیل این کار جلوگیری از خطا در مصورسازی انتهایی است.<br>\n",
    "سپس با توجه به زمانبر بودن اجرا،حداقل یک سمپل 1500 تایی نمونه برداری کردیم.\n",
    "<br>\n",
    "برای کسب نتایج بهتر و مصورسازی مناسبتر میتوانیم تعداد سمپل را افزایش دهیم.\n",
    "</p>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mG_ng9hsUMGc"
   },
   "outputs": [],
   "source": [
    "df_10k = pd.read_csv('10k_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CppfUdF3Zyp8"
   },
   "outputs": [],
   "source": [
    "df_10k.fillna(value=\" \",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehfaa_-eUMGd"
   },
   "outputs": [],
   "source": [
    "df = df_10k.sample(1500, random_state=42)\n",
    "del df_10k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSr5HP2wUMGg"
   },
   "source": [
    "<font face=\"B Mitra\">\n",
    "<div dir=rtl>\n",
    "<font size=6>\n",
    "<b> پیش پردازش متن</b>\n",
    "<br>\n",
    "<p align=\"justify\">\n",
    "<font size=5>\n",
    " برای پیش پردازش متون از کتابخانه های مختلفی از جمله NLTK ،Gensim یا Spacy میتوان استفاده کرد.\n",
    " <br>\n",
    " مواردی که در پیش پردازش متون باید به آن توجه کرد.\n",
    " <ol>\n",
    " <br>\n",
    " <li> <b>تشخیص زبان مورداستفاده مقاله ها:</b>\n",
    "  در این مجموعه داده میتوانید  با استفاده از کتابخانه langdetect زبان مورد استفاده مقاله را در ویژگی <code>body_text</code> بررسی کنید.\n",
    " با توجه به نتیجه نهایی خواهید دید که زبان انگلیسی زبان غالب در این مقاله هاست بنابراین میتوان مقاله های غیر انگلیسی را حذف کرد.\n",
    " </li>\n",
    " <li> <b>توکنایز کردن (Tokenize) مدل ها:</b>\n",
    " در این مجموعه داده از  پایپ لاین  <code>en_core_sci_lg</code> از کتابخانه  spaCy به دلیلی پشتبانی از داده های زیستی و پزشکی و با بردارهای 600 هزار کلمه ای و واژگان بزرگتر میتوانیم استفاده کنیم.<br>\n",
    " به دلیل اینکه SpaCy حداکثر یک میلیون کاراکتر را بررسی میکند میتوان با استفاده از max_length این محدودیت را کاهش داد:\n",
    " لطفا از parser مشخص شده در بخش زیر برای توکنایز کردن متن خود استفاده کنید.\n",
    "\n",
    "\n",
    " </li>\n",
    " <li> <b> Lemmatization یا Stemming (ریشه یابی) </b>\n",
    " </li>\n",
    " <li> <b>حذف Stopwords</b> </li>\n",
    " <li> <b>حذف علائم نشانه گذاری</b> </li>\n",
    " </ol>\n",
    "<br>\n",
    "درمورد هر یک از موارد بالا جستجو کنید و پیش پردازش های لازم را برای ویژگی \"body text\" مقاله انجام دهید.\n",
    "</p>\n",
    "</font>\n",
    "</div>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EEVTLXxFD9hC"
   },
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "import en_core_sci_lg\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# تشخیص زبان مورد استفاده\n",
    "def detect_language(text):\n",
    "    return detect(text)\n",
    "\n",
    "# توکن‌بندی\n",
    "def tokenize_text(text):\n",
    "    parser = en_core_sci_lg.load(disable=[\"tagger\", \"ner\"])\n",
    "    parser.max_length = 3000000\n",
    "    doc = parser(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    return tokens\n",
    "\n",
    "# ریشه‌یابی\n",
    "def stem_text(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "# حذف stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "# حذف علائم نشانه‌گذاری\n",
    "def remove_punctuation(text):\n",
    "    clean_text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "# نمونه استفاده\n",
    "text = \"متن مقاله\"\n",
    "\n",
    "# تشخیص زبان متن\n",
    "language = detect_language(text)\n",
    "print(\"زبان متن:\", language)\n",
    "\n",
    "# توکن‌بندی متن\n",
    "tokens = tokenize_text(text)\n",
    "print(\"توکن‌ها:\", tokens)\n",
    "\n",
    "# ریشه‌یابی توکن‌ها\n",
    "stemmed_tokens = stem_text(tokens)\n",
    "print(\"توکن‌های ریشه‌یابی شده:\", stemmed_tokens)\n",
    "\n",
    "# حذف stopwords از توکن‌ها\n",
    "filtered_tokens = remove_stopwords(tokens)\n",
    "print(\"توکن‌های پاک شده از stopwords:\", filtered_tokens)\n",
    "\n",
    "# حذف علائم نشانه‌گذاری\n",
    "clean_text = remove_punctuation(text)\n",
    "print(\"متن پاک شده از علائم نشانه‌گذاری:\", clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Ks9shj8UMGm"
   },
   "source": [
    "<font face=\"B Mitra\">\n",
    "<div dir=rtl>\n",
    "<font size=6>\n",
    "<b> استخراج ویژگی </b>\n",
    "<br>\n",
    "<p align=\"justify\">\n",
    "<font size=5>\n",
    "اکنون که متن بدنهٔ مقاله‌ها را از قبل پردازش کرده‌ایم، زمان تبدیل آن‌ها به قالبی است که توسط الگوریتم‌های ما قابل استفاده باشد. برای این منظور از tf-idf استفاده خواهیم کرد. tf_idf  یک الگوریتم بسیار رایج برای تبدیل متن به نمایش معنی دار اعداد است که اهمیت هر کلمه را در متن موردنظر نشان میدهد. <br>\n",
    "درمورد عملکرد این روش جستجو کنید.\n",
    " برای متن پیش پردازش شده هر مقاله، با استفاده از این روش یک بازنمایی برداری با حداکثر 4096 ویژگی ایجاد کنید.\n",
    "</p>\n",
    "</font>\n",
    "</div>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bnzpP2cW7Q2z"
   },
   "outputs": [],
   "source": [
    "def extract_tfidf_features(texts):\n",
    "    # ایجاد شیء TfidfVectorizer با حداکثر 4096 ویژگی\n",
    "    vectorizer = TfidfVectorizer(max_features=4096)\n",
    "    \n",
    "    # استخراج ویژگی‌ها با استفاده از TF-IDF\n",
    "    features = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# تبدیل متن پیش‌پردازش‌شده به بردارهای ویژگی با استفاده از TF-IDF\n",
    "preprocessed_texts = df['preprocessed_text'].tolist()\n",
    "features = extract_tfidf_features(preprocessed_texts)\n",
    "\n",
    "# نمایش ابعاد ماتریس ویژگی‌ها\n",
    "print(\"ابعاد ماتریس ویژگی‌ها:\", features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkE2ydTrUMGm"
   },
   "source": [
    "<font face=\"B Mitra\"><div dir=rtl>\n",
    "<font size=6>\n",
    "<b> PCA </b>\n",
    "<br>\n",
    "<p align=\"justify\">\n",
    "<font size=5>\n",
    "با توجه به ابعاد بالای هر بردار ویژگی، با استفاده از روش PCA با حفظ 95 درصد واریانس کاهش بعد دهید.\n",
    "</p>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nn7maPOiEen9"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# تعداد اجزای اصلی مورد نیاز برای حفظ 95% واریانس\n",
    "n_components = 0.95\n",
    "\n",
    "# ایجاد شیء PCA با تعداد اجزای اصلی مورد نیاز\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "# کاهش بعد بردارهای ویژگی با استفاده از PCA\n",
    "reduced_features = pca.fit_transform(features.toarray())\n",
    "\n",
    "# نمایش ابعاد بردارهای ویژگی کاهش یافته\n",
    "print(\"ابعاد بردارهای ویژگی کاهش یافته:\", reduced_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWtyVDVwBrAA"
   },
   "source": [
    "<font face=\"B Mitra\">\n",
    "<div dir=rtl>\n",
    "<font size=6>\n",
    "<b> خوشه بندی </b>\n",
    "<br>\n",
    "<p align=\"justify\">\n",
    "<font size=5>\n",
    "خوشه بندی مقالات تحقیقاتی مشابه با هم می تواند جستجوی انتشارات مرتبط را ساده کند.  در اینجا از روش K-Means برای خوشه بندی استفاده میکنیم.<br>\n",
    " با استفاده از روش Elbow Method  از بین تعداد خوشه های داده شده، میتوانید بهترین تعداد خوشه را بیابید.\n",
    "<br>\n",
    "تا 30 خوشه این موضوع را مورد بررسی قرار دهید و پس از یافتن تعداد خوشه مناسب، با استفاده از روش K-Means خوشه بندی لازم را برای بردار ویژگی\n",
    "بدست آماده از مرحلهٔ قبل به کار بگیرید.\n",
    "<br><b> بخش امتیازی:</b> میتوانید با به کارگیری روش های دیگر خوشه بندی نظیر خوشه بندی سلسه مراتبی یا DBSCAN نتایج را با استفاده از یک معیار ارزیابی مناسب  مقایسه کنید.\n",
    "</p>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yllwsZlaJKCD"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# تعداد خوشه‌های مورد بررسی\n",
    "max_clusters = 30\n",
    "\n",
    "# لیست برای ذخیره واریانس داخل خوشه‌ها\n",
    "inertia = []\n",
    "\n",
    "# انجام خوشه‌بندی با تعداد خوشه‌های مختلف\n",
    "for n_clusters in range(1, max_clusters+1):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(reduced_features)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# رسم نمودار Elbow Method\n",
    "plt.plot(range(1, max_clusters+1), inertia, marker='o')\n",
    "plt.xlabel('تعداد خوشه‌ها')\n",
    "plt.ylabel('واریانس داخل خوشه‌ها')\n",
    "plt.title('Elbow Method')\n",
    "plt.show()\n",
    "\n",
    "# تحلیل نمودار Elbow Method\n",
    "distances = []\n",
    "for i in range(1, len(inertia)):\n",
    "    distances.append(inertia[i-1] - inertia[i])\n",
    "\n",
    "# پیدا کردن نقطه انحنای نمودار\n",
    "elbow_index = np.argmax(distances) + 1\n",
    "best_n_clusters = elbow_index\n",
    "\n",
    "# چاپ تعداد بهترین خوشه‌ها\n",
    "print(\"تعداد بهترین خوشه‌ها:\", best_n_clusters)\n",
    "\n",
    "# اجرای خوشه‌بندی با تعداد بهترین خوشه‌ها\n",
    "kmeans = KMeans(n_clusters=best_n_clusters, random_state=42)\n",
    "kmeans.fit(reduced_features)\n",
    "\n",
    "# برچسب‌های خوشه‌ها\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# نمایش تعداد نمونه‌ها در هر خوشه\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "cluster_counts = dict(zip(unique, counts))\n",
    "print(\"تعداد نمونه‌ها در هر خوشه:\", cluster_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NcEptMmUMGn"
   },
   "source": [
    "<font face=\"B Mitra\">\n",
    "<div dir=rtl>\n",
    "<font size=6>\n",
    "<b> t-SNE </b>\n",
    "<br>\n",
    "<p align=\"justify\">\n",
    "<font size=5>\n",
    "t_SNE یک روش نظارت نشده غیر خطی است که برای اکتشاف و بصری‌سازی داده‌ها مورد استفاده قرار می‌گیرد.\n",
    "PCA یک روش کاهش ابعاد خطی است که در تلاش برای بیشینه کردن واریانس و حفظ فاصله‌های زیاد دوتایی‌ها از یکدیگر است. این امر می‌تواند منجر به بصری‌سازی ضعیف به ویژه هنگام کار با ساختارهای غیرخطی می‌شود. t-SNE  با حفظ فاصله‌های کم دوتایی‌ها یا شباهت محلی از PCA متمایز می‌شود. به بیان ساده‌تر، t-SNE به کاربر درکی از اینکه داده‌ها چگونه در فضای ابعاد بالا سازمان‌دهی شده‌اند را ارائه می‌کند. <br>\n",
    "با استفاده از t-SNE می توانیم بردار ویژگی هایی با ابعاد بالا را به 2 بعد کاهش دهیم.\n",
    "در این مجموعه داده در 2 بعد می توان توزیع مقالات را با استفاده از scatterplot نمایش داد. <br>\n",
    "برای تفکیک بصری موضوعات مختلف در نمودار بالا، با استفاده از خوشه هایی که در KMeans یافتید ، برای رنگ آمیزی خوشه های مختلف در مصورسازی خود میتوانید استفاده کنید.\n",
    "</p>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZMLknF7HJ1qH"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# اعمال t-SNE بر روی بردار ویژگی‌های با کاهش ابعاد\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_features = tsne.fit_transform(reduced_features)\n",
    "\n",
    "# خوشه‌بندی با استفاده از KMeans\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "kmeans.fit(reduced_features)\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# نمایش scatterplot و استفاده از رنگ‌آمیزی براساس خوشه‌ها\n",
    "plt.scatter(tsne_features[:, 0], tsne_features[:, 1], c=cluster_labels)\n",
    "plt.title('t-SNE Scatterplot')\n",
    "plt.xlabel('محور اول t-SNE')\n",
    "plt.ylabel('محور دوم t-SNE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwJk9jloUMGt"
   },
   "source": [
    "<font face=\"B Mitra\">\n",
    "<div dir=rtl>\n",
    "<font size=6>\n",
    "<b> مدلسازی موضوعی (Topic Modeling)</b>\n",
    "<br>\n",
    "<p align=\"justify\">\n",
    "<font size=5>\n",
    "اکنون سعی خواهیم کرد موضوعات کلیدی در هر خوشه را پیدا کنیم.\n",
    "<br>\n",
    " K-means مقالات را خوشه بندی کرد اما موضوعات را برچسب گذاری نکرد. از طریق مدل‌سازی موضوع، متوجه خواهیم شد که مهمترین اصطلاحات برای هر خوشه چیست. این کار با دادن کلمات کلیدی برای شناسایی سریع مضامین خوشه، معنای بیشتری به خوشه می بخشد.<br>\n",
    "برای مدلسازی موضوعی از الگوریتم های مختلفی نظیر NMF,LSA,LDA میتوان استفاده کرد.\n",
    "<br>\n",
    "تعداد موضوعاتی که از خوشه ها قصد دارید استخراج کنید، را به دلخواه انتخاب کنید.\n",
    "<br>\n",
    "با استفاده از روش LDA موضوعات مناسب را به تعداد موردنظر برای هر خوشه بیابید.\n",
    "سپس کلمات کلیدی هر موضوع برای خوشه موردنظر را چاپ کنید.\n",
    "<br><b> بخش امتیازی:</b> مدلسازی موضوعی با روشی غیر از LDA انجام دهید و موضوعات خوشه ها را در هردو روش با هم مقایسه کنید.\n",
    "\n",
    "</p>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "By4_qAM_C_p0"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# تعداد موضوعات موردنظر برای هر خوشه\n",
    "num_topics_per_cluster = 5\n",
    "\n",
    "# آموزش مدل LDA و استخراج موضوعات برای هر خوشه\n",
    "for cluster_id in range(num_clusters):\n",
    "    # فیلتر کردن مقالات متعلق به خوشه فعلی\n",
    "    cluster_articles = articles[cluster_labels == cluster_id]\n",
    "    \n",
    "    # ایجاد بردار ویژگی‌ها برای مدل LDA\n",
    "    vectorizer = TfidfVectorizer(max_df=0.8, min_df=2, stop_words='english')\n",
    "    tfidf = vectorizer.fit_transform(cluster_articles)\n",
    "    \n",
    "    # آموزش مدل LDA\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics_per_cluster, random_state=42)\n",
    "    lda.fit(tfidf)\n",
    "    \n",
    "    # استخراج کلمات کلیدی برای هر موضوع\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for topic_id, topic in enumerate(lda.components_):\n",
    "        topic_keywords = [feature_names[i] for i in topic.argsort()[:-6:-1]]  # 5 کلمه کلیدی برتر\n",
    "        print(f\"خوشه {cluster_id} - موضوع {topic_id}: {', '.join(topic_keywords)}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# تعداد موضوعات موردنظر برای هر خوشه\n",
    "num_topics_per_cluster = 5\n",
    "\n",
    "# آموزش مدل NMF و استخراج موضوعات برای هر خوشه\n",
    "for cluster_id in range(num_clusters):\n",
    "    # فیلتر کردن مقالات متعلق به خوشه فعلی\n",
    "    cluster_articles = articles[cluster_labels == cluster_id]\n",
    "    \n",
    "    # ایجاد بردار ویژگی‌ها برای مدل NMF\n",
    "    vectorizer = TfidfVectorizer(max_df=0.8, min_df=2, stop_words='english')\n",
    "    tfidf = vectorizer.fit_transform(cluster_articles)\n",
    "    \n",
    "    # آموزش مدل NMF\n",
    "    nmf = NMF(n_components=num_topics_per_cluster, random_state=42)\n",
    "    nmf.fit(tfidf)\n",
    "    \n",
    "    # استخراج کلمات کلیدی برای هر موضوع\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for topic_id, topic in enumerate(nmf.components_):\n",
    "        topic_keywords = [feature_names[i] for i in topic.argsort()[:-6:-1]]  # 5 کلمه کلیدی برتر\n",
    "        print(f\"خوشه {cluster_id} - موضوع {topic_id}: {', '.join(topic_keywords)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLO8IzbODDry"
   },
   "source": [
    "<font face=\"B Mitra\">\n",
    "<div dir=rtl>\n",
    "<font size=6>\n",
    "<b>  مصورسازی </b>\n",
    "<br>\n",
    "<p align=\"justify\">\n",
    "<font size=4>\n",
    "<b>این بخش شامل نمره نیست و صرفا جهت درک شما از مراحل انجام شده است</b><br>\n",
    "<font size=5>\n",
    "مراحل قبلی برچسب‌های خوشه‌بندی و مجموعه‌ای از مقالات را به دو بعد کاهش داده است. می‌توانیم یک طرح تعاملی از خوشه ها ایجاد کنیم. <br>\n",
    "شما میتوانید با انتخاب هر خوشه از اسلایدر، توزیع هر خوشه را به صورت مجزا مشاهده کنید. همچنین میتوانید با بردن نشانگر روی هریک از نقاط مشخصات کلی آن مقاله را مشاهده کنید.\n",
    "<br>\n",
    "<img src=\"https://drive.google.com/uc?id=14xXSuD-FhmSSJBI0oH-a-DDkgjBtqj4_\" alt=\"Linear Algebra Cover Art\" width=\"800\">\n",
    "</p>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5O_BzL6Z3Nq"
   },
   "source": [
    "<font face=\"B Mitra\">\n",
    "<div dir=rtl>\n",
    "<p align=\"justify\">\n",
    "<font size=5>\n",
    "ورودی تابع زیر، دیتافریم مقالات می‌باشد که می‌بایست شامل ستون‌های زیر باشد:\n",
    "<br>\n",
    "1. <code>tsne-2d-one</code>: مولفهٔ اول t-SNE\n",
    "<br>\n",
    "2. <code>tsne-2d-two</code>: مولفهٔ دوم t-SNE\n",
    "<br>\n",
    "3. <code>y</code>: خوشهٔ هر یک از مقالات\n",
    "</p>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FboibFCYLKAW"
   },
   "outputs": [],
   "source": [
    "def interactive_plot(df):\n",
    "    clusters = sorted(df['y'].unique())\n",
    "\n",
    "    # Generate distinct colors for each cluster using Plotly's colors\n",
    "    colors = px.colors.qualitative.Plotly\n",
    "\n",
    "    # Create a scatter plot for each cluster with a unique color\n",
    "    data = []\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        cluster_data = df[df['y'] == cluster]\n",
    "        scatter = go.Scatter(\n",
    "            x=cluster_data['tsne-2d-one'],\n",
    "            y=cluster_data['tsne-2d-two'],\n",
    "            mode='markers',\n",
    "            name=f'Cluster {cluster}',\n",
    "            text=cluster_data['title'],\n",
    "            hoverinfo='text',\n",
    "            marker=dict(size=10, color=colors[i % len(colors)]),  # Assign a unique color\n",
    "            visible=False  # Initially, make all traces invisible\n",
    "        )\n",
    "        data.append(scatter)\n",
    "\n",
    "    # Add a scatter plot for all clusters with different colors\n",
    "    scatter_all = go.Scatter(\n",
    "        x=df['tsne-2d-one'],\n",
    "        y=df['tsne-2d-two'],\n",
    "        mode='markers',\n",
    "        name='All Clusters',\n",
    "        text=df['title'],\n",
    "        hoverinfo='text',\n",
    "        marker=dict(size=10, color=df['y'].map(lambda x: colors[x % len(colors)])),  # Assign colors by cluster\n",
    "        visible=True  # Initially, show all clusters\n",
    "    )\n",
    "    data.append(scatter_all)\n",
    "\n",
    "    x_min = df['tsne-2d-one'].min()\n",
    "    x_max = df['tsne-2d-one'].max()\n",
    "    y_min = df['tsne-2d-two'].min()\n",
    "    y_max = df['tsne-2d-two'].max()\n",
    "\n",
    "    # Initialize the figure\n",
    "    fig = go.Figure(data=data)\n",
    "\n",
    "    # Ensure square aspect ratio\n",
    "    fig.update_layout(\n",
    "        title='Cluster Visualization',\n",
    "        xaxis=dict(title='t-SNE 1', range=[x_min, x_max]),  # Ensure x and y axes are equal\n",
    "        yaxis=dict(title='t-SNE 2', range=[y_min, y_max]),\n",
    "        width=1000,\n",
    "        height=700  # Ensure the figure is square-shaped\n",
    "    )\n",
    "\n",
    "    # Add hover functionality\n",
    "    fig.update_traces(\n",
    "        hoverinfo='text',\n",
    "        marker=dict(opacity=0.7, size=8),\n",
    "    )\n",
    "\n",
    "    # Add interactive slider for cluster selection\n",
    "    steps = []\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        step = dict(\n",
    "            method='update',\n",
    "            args=[{'visible': [False] * len(clusters) + [False]},  # Hide all clusters\n",
    "                {'title': f'Cluster {cluster}'}],\n",
    "            label=f'Cluster {cluster}'\n",
    "        )\n",
    "        # Only make the current cluster visible\n",
    "        step['args'][0]['visible'][i] = True\n",
    "        steps.append(step)\n",
    "\n",
    "    # Add final step for all clusters\n",
    "    steps.append(dict(\n",
    "        method='update',\n",
    "        args=[{'visible': [False] * len(clusters) + [True]},  # Only show the 'all clusters' trace\n",
    "            {'title': 'All Clusters'}],\n",
    "        label='All Clusters'\n",
    "    ))\n",
    "\n",
    "    sliders = [dict(\n",
    "        active=len(steps) - 1,\n",
    "        currentvalue={\"prefix\": \"Cluster: \"},\n",
    "        pad={\"t\": 50},\n",
    "        steps=steps\n",
    "    )]\n",
    "\n",
    "    fig.update_layout(\n",
    "        sliders=sliders\n",
    "    )\n",
    "\n",
    "    # Display the plot\n",
    "    fig.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
